{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6613040f-67c9-4e58-a416-0aff6e066651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/emmabarnes/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmabarnes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from os import getcwd\n",
    "#import w1_unittest\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8be310-c451-4ee6-9c12-662734c0a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91543d1e-92bb-446f-9165-3385abca9b93",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/emmabarnes/.local/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /Users/emmabarnes/.local/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/emmabarnes/.local/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ce8127-aa6d-433e-b5de-db8b4aaad1e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d7157c-fcf9-4215-a0da-5dbf0cd5782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fee85c7-3af9-4e19-a11e-543668a4eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6755d44-e152-4517-b583-17b83cb0b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a916f0-55ee-4750-a284-8def850c4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation): \n",
    "        # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc13a85-f246-4654-ab02-a57b81ae28f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2726a291-8e4f-4a39-93a2-4861558e0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos = twitter_samples.strings('positive_tweets.json')\n",
    "all_neg = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "068f01bf-e60c-4f4e-ae50-33210b45bf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.reader.twitter.TwitterCorpusReader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(twitter_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f59c246c-b946-4928-955a-945b05b29c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c00e12-f8ca-4da7-91e5-a2dcc4e32f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = all_pos[4000:]\n",
    "train_pos = all_pos[:4000]\n",
    "test_neg = all_neg[4000:]\n",
    "train_neg = all_neg[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4212182-f53c-42ba-b935-c88f384d3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262a0cb-05df-4f5f-817c-6bbdc3dd3afb",
   "metadata": {},
   "source": [
    " - numpy array of all positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c27ec1e-d988-462e-a4d7-0e97b2e6f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)),axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb7d4e67-b8c7-4f01-83d3-b8961dfc923a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "#test split should be 20% train set should be 80%\n",
    "\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a62a29-4b7f-443d-ab43-49e811b1f890",
   "metadata": {},
   "source": [
    " - create frequency dictionary with the build_freqs thing\n",
    " - the outer loop goes through each text body, the inner loop through each word in the text\n",
    " - the key is the tuple (word, label) such as (\"happy\", 1) or (\"happy\", 0). Value stored for each key is the count of how many times the word happy gets associated with a positive/negative label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0febab7d-4fef-4421-9295-6635be28dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freqs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd215c0f-10c7-4cc6-949a-f6dbe2e8d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type freqs = <class 'dict'>\n",
      "length freqs = 11338\n"
     ]
    }
   ],
   "source": [
    "print(\"type freqs = \" + str(type(freqs)))\n",
    "print(\"length freqs = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc909e8-45e3-457e-bd3a-c0f51758cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8bf1eb9-4619-4bbe-9e57-13b80a118846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88e553-0498-4f83-a958-33b488a1a050",
   "metadata": {},
   "source": [
    " - sigmoid maps z to a value ranging between 0 and 1, so it can be treated as a probability\n",
    " - this function should work if z is a scalar or an array representing a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88a77813-d86b-4970-9b66-6912d421c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    h = 1/(1+np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def53f4c-8ded-42e2-9c3f-0b294c4e1efc",
   "metadata": {},
   "source": [
    " - the logic of logistic regression is taking a regular linear regression and applying a sigmoid to the output\n",
    " - regression: \n",
    "     z = theta0X0 + theta1X1 + ... + thetaNXN\n",
    "     theta - weights (w vector)\n",
    " - logistic regression:\n",
    "     h(z) = 1/1+exp^(-z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c368ad4-6c13-4b54-a904-ab7532b95599",
   "metadata": {},
   "source": [
    " The cost function used for log reg is the average of the log loss across all the training examples: function J(theta)\n",
    " - where m is the number of training examples\n",
    " - y^(i) is the actual label of training example 'i'\n",
    " - h(z^(i)) is the model's prediction for the training example 'i'\n",
    " \n",
    " the loss function Loss for a single training example is:\n",
    "     Loss = -1 X (y^(i)log(h(z(theta)^i))+(1-y^i)log(1-h(z(theta)^i)))\n",
    " - all the h values are between 0 and 1, so logs will be negative. This is why we apply factor of -1 to the sum of the two loss terms\n",
    " - when the model predicts 1 (h(z(theta)) = 1) and the label 'y' is also 1, the loss for that training example is 0. \n",
    " - when the model predicts 0 (h(z(theta)) = 0) and the actual label is also 0, the loss for that training example is 0.\n",
    " - When the model prediction is close to 1 (h(z(theta)) = 0.9999) and the label is 0, the second term of the log loss becomes a large negative number which is multiplied by the overall factor of -1 to convert it to a positive loss value. The closer the model prediction gets to 1, the greater the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a75964-d9a3-4d57-bd18-340c2d170485",
   "metadata": {},
   "source": [
    "To update weight/parameter vector theta, apply gradient descent to iteratively improve model predictions by weight. \n",
    "The gradient of the cost function J with respect to one weight theta(sub j).\n",
    " - i is the index across all m training examples\n",
    " - j is the index of the weight theta sub j ,so that x^i sub j is the feature associated with the weight theta sub j\n",
    " - to update the weight theta sub j, adjust by subtracting a fraction of the gradient determined by alpha. \n",
    " theta(sub j) = theta(sub j) - alpha X delta(sub theta (sub j))J(theta)\n",
    " - the LEARNING RATE alpha is a value WE CHOOSE to control how big a single update will be for each pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93722f-2313-49d3-9676-291472ff5d61",
   "metadata": {},
   "source": [
    "For the gradient descent function...\n",
    "\n",
    " - the number of iterations num_iters is the number of times you'll use the entire training set\n",
    " - for each iteration, calculate the cost function using all training examples ('m' training examples), and for all features\n",
    " - instead of updating single weight theta(sub i) at a time, apply updates to the weights in the column vector theta for all theta(sub n)\n",
    " \n",
    " - theta has dimensions(n+1, 1) where 'n' is the number of features, and there is one more element for the bias term theta(sub 0) the corresponding feature value x(sub 0) is just 1. \n",
    " - The z are calculated by multiplying the feature matrix 'x' with the weight vector 'theta' z = x*theta\n",
    "     - x has dimensions (m, n+1)\n",
    "     - theta has dimensions (n+1, 1)\n",
    "     - z has dimensions (m, 1)\n",
    " - The prediction h is calculated by applying the sigmoid to each element in 'z': h(z) = sigmoid(z), and has dimensions (m, 1)\n",
    " - the cost function J is calculated by taking the dot product of the vectors 'y' and 'log(h)'. Since both y and h are column vectors of size (m, 1), transpose the vector to the LEFT so that the matrix multiplication of a row vector with a column vector handles the dot product. (T in my equations are transpose, i'm doing vector stuff :D)\n",
    " - the update of theta is also vectorized. Since dimensions of x are (m, n+1), and both h and y are (m, 1), we need to transpose the x and move it left to preform the matrix multiplication wiht yields (n+1, 1) result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fa2fa9a-cb8b-4b4d-9a1c-a978ce75a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient (x, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    for i in range(0, num_iters):\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "        J = (-1/m)*np.sum(y*np.log(h)+(1-y)*np.log(1-h))\n",
    "        theta = theta-(alpha/m)*np.dot(x.T, (h-y))\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ed2e26-3665-4a71-9e6c-ee3b97b3d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost0.6709497038162117\n",
      "weight vector [4.1e-07, 0.00035658, 7.309e-05]\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "np.random.seed(1)\n",
    "#X input is 10 by 3 dim with 1s for biases\n",
    "tmp_X = np.append(np.ones((10,1)), np.random.rand(10,2)*2000, axis=1)\n",
    "#Y labels are 10 by 1\n",
    "tmp_Y = (np.random.rand(10,1)>0.35).astype(float)\n",
    "\n",
    "#with gradient\n",
    "tmp_J, tmp_theta = gradient(tmp_X, tmp_Y, np.zeros((3,1)), 1e-8, 700)\n",
    "print(\"cost\" + str(tmp_J))\n",
    "print(\"weight vector \" + str([round(t, 8) for t in np.squeeze(tmp_theta)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3327860-d0c5-4b00-8005-df24ac28eb21",
   "metadata": {},
   "source": [
    " - given a list of reviews/text extract the features and put them in a matrix, extract two \n",
    "     - positive (illicit)\n",
    "     - negative (non illicit)\n",
    " - train logistic classifier on these features\n",
    " - test on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebb9117-668f-4dc1-9113-536aa5404249",
   "metadata": {},
   "source": [
    " - function takes a single text body\n",
    " - processes text\n",
    " - loops through each word of processed words\n",
    "     - for each word, check freqs dictionary for the count when that word has a positive '1' label(check for key word (word, 1.0\n",
    "     - do the same for when the word is associated wiht negative label 0 \n",
    "NOTE: the prediciton being positive or negative depends on feature vector which counts-in duplicate words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f370481-041a-4c8b-9f7b-2987a74020a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(tweet, freqs, process_tweet=process_tweet):\n",
    "    word_l = process_tweet(tweet)\n",
    "    x = np.zeros(3)\n",
    "    x[0] = 1\n",
    "    \n",
    "    for word in word_l:\n",
    "        x[1] += freqs.get((word, 1), 0) #positive label\n",
    "        x[2] += freqs.get((word, 0), 0) #neg label\n",
    "    x = x[None, :]\n",
    "    assert(x.shape == (1,3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1746c8ec-0792-48ce-966e-95785c7465e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e+00 3.02e+03 6.10e+01]]\n"
     ]
    }
   ],
   "source": [
    "#first test on training data\n",
    "tmp1 = extractFeatures(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80afb5f-bf45-42ce-bfec-41a27b734cad",
   "metadata": {},
   "source": [
    "to train the model...\n",
    " - stack the features for all training examples into a matrix X\n",
    " - call the gradient function to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56b17bdd-efbe-4579-ae49-257446957bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost of training0.24215473924831094\n",
      "vector weights[7e-08, 0.00052391, -0.00055517]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(train_x), 3)) #collects features x and stacks to matrix X\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extractFeatures(train_x[i], freqs)\n",
    "    \n",
    "Y = train_y #training labels for X\n",
    "\n",
    "#apply gradient\n",
    "J, theta = gradient(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "\n",
    "print(\"cost of training\" + str(J))\n",
    "print(\"vector weights\" + str([round(t, 8) for t in np.squeeze(theta)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6a671-a06e-41b7-a304-5a91757681a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e045e0f-fd96-42c4-9981-e440765aea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tweet, freqs, theta):\n",
    "    x = extractFeatures(tweet, freqs)\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a3e46b4-fd21-40bc-badc-61a4b3e6fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy === [[0.51858054]]\n",
      "I am sad === [[0.48677862]]\n",
      "this sucks === [[0.49827197]]\n",
      "this is great === [[0.51546416]]\n",
      "great === [[0.51546416]]\n",
      "great great === [[0.53089874]]\n"
     ]
    }
   ],
   "source": [
    "#testing output\n",
    "for tweet in ['I am happy', 'I am sad', 'this sucks', 'this is great', 'great', 'great great']:\n",
    "    print(str(tweet) + \" === \" + str(predict(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54196dea-a428-47a0-9553-8f1116984d9e",
   "metadata": {},
   "source": [
    "After training the model check how it will work on unseen data by testing against test set\n",
    "\n",
    " - given test data and weights of trained mode, calculate accuracy\n",
    " - use predict to make predictions on each in the test set\n",
    " - if the prediction is >0.5 set the model's classification y_hat to 1, otherwise 0.\n",
    " - the prediction is accurate when y_hat = test_y. Sum the cases when this happens and divide by m to get average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3b83a29-aaa9-433b-8ff5-7de91b9f993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy (test_x, test_y, freqs, theta, predict=predict):\n",
    "    y_hat = []\n",
    "    for tweet in test_x:\n",
    "        y_pred = predict(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5: #good prediction\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            y_hat.append(0.0)\n",
    "            \n",
    "    accuracy = np.sum(y_hat==test_y.squeeze())/len(test_x)#merge into a 1d array with == to compare\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5275c03-c5fe-4ed2-a2ec-820d16e8132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.995\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = testAccuracy(test_x, test_y, freqs, theta)\n",
    "print(\"accuracy: \" + str(tmp_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c0b9219-221a-4e1b-be34-73c873597e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realli', 'good', 'thing', 'love', 'thing', 'end', 'terribl', '..']\n",
      "[[0.53101143]]\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "test = 'this is a really good thing. I loved this thing but the ending was terrible..'\n",
    "print(process_tweet(test))\n",
    "y_hat = predict(test, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat>0.5:\n",
    "    print(\"POSITIVE\")\n",
    "else:\n",
    "    print(\"NEGATIVE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
